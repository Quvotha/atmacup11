{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T20:08:16.954305Z",
     "iopub.status.busy": "2021-07-18T20:08:16.953299Z",
     "iopub.status.idle": "2021-07-18T20:08:16.958812Z",
     "shell.execute_reply": "2021-07-18T20:08:16.958812Z"
    }
   },
   "outputs": [],
   "source": [
    "EXP_NO = 17\n",
    "SEED = 1\n",
    "N_SPLITS = 5\n",
    "TARGET = 'target'\n",
    "GROUP = 'art_series_id'\n",
    "REGRESSION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T20:08:16.958812Z",
     "iopub.status.busy": "2021-07-18T20:08:16.958812Z",
     "iopub.status.idle": "2021-07-18T20:08:16.974446Z",
     "shell.execute_reply": "2021-07-18T20:08:16.974446Z"
    }
   },
   "outputs": [],
   "source": [
    "assert((TARGET, REGRESSION) in (('target', True), ('target', False), ('sorting_date', True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'inception'\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T20:08:16.974446Z",
     "iopub.status.busy": "2021-07-18T20:08:16.974446Z",
     "iopub.status.idle": "2021-07-18T20:08:20.307818Z",
     "shell.execute_reply": "2021-07-18T20:08:20.307818Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "from logging import getLogger, StreamHandler, FileHandler, DEBUG, Formatter\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, cohen_kappa_score\n",
    "from sklearnex import patch_sklearn\n",
    "from pytorch_lightning import seed_everything\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "SCRIPTS_DIR = os.path.join('..', 'scripts')\n",
    "assert(os.path.isdir(SCRIPTS_DIR))\n",
    "if SCRIPTS_DIR not in sys.path: sys.path.append(SCRIPTS_DIR)\n",
    "\n",
    "from cross_validation import load_cv_object_ids\n",
    "from dataset import load_csvfiles, load_photofile,load_photofiles, AtmaImageDatasetV02\n",
    "from folder import experiment_dir_of\n",
    "from models import initialize_model\n",
    "from utils import train_model, predict_by_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T20:08:20.307818Z",
     "iopub.status.busy": "2021-07-18T20:08:20.307818Z",
     "iopub.status.idle": "2021-07-18T20:08:20.323443Z",
     "shell.execute_reply": "2021-07-18T20:08:20.323443Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.5f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T20:08:20.323443Z",
     "iopub.status.busy": "2021-07-18T20:08:20.323443Z",
     "iopub.status.idle": "2021-07-18T20:08:20.339068Z",
     "shell.execute_reply": "2021-07-18T20:08:20.339068Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = experiment_dir_of(EXP_NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T20:08:20.339068Z",
     "iopub.status.busy": "2021-07-18T20:08:20.339068Z",
     "iopub.status.idle": "2021-07-18T20:08:20.354692Z",
     "shell.execute_reply": "2021-07-18T20:08:20.354692Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T20:08:20.354692Z",
     "iopub.status.busy": "2021-07-18T20:08:20.354692Z",
     "iopub.status.idle": "2021-07-18T20:08:20.370316Z",
     "shell.execute_reply": "2021-07-18T20:08:20.370316Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T20:08:20.370316Z",
     "iopub.status.busy": "2021-07-18T20:08:20.370316Z",
     "iopub.status.idle": "2021-07-18T20:08:20.385940Z",
     "shell.execute_reply": "2021-07-18T20:08:20.385940Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Refference\n",
    "https://docs.python.org/ja/3/howto/logging-cookbook.html\n",
    "'''\n",
    "logger.setLevel(DEBUG)\n",
    "# create file handler which logs even debug messages\n",
    "fh = FileHandler(os.path.join(output_dir, 'log.log'))\n",
    "fh.setLevel(DEBUG)\n",
    "# create console handler with a higher log level\n",
    "ch = StreamHandler()\n",
    "ch.setLevel(DEBUG)\n",
    "# create formatter and add it to the handlers\n",
    "formatter = Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "# add the handlers to the logger\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)\n",
    "len(logger.handlers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T20:08:20.385940Z",
     "iopub.status.busy": "2021-07-18T20:08:20.385940Z",
     "iopub.status.idle": "2021-07-18T20:08:20.401565Z",
     "shell.execute_reply": "2021-07-18T20:08:20.401565Z"
    }
   },
   "outputs": [],
   "source": [
    "logger.info('Experiment no: {}'.format(EXP_NO))\n",
    "logger.info('CV: StratifiedGroupKFold')\n",
    "logger.info('SEED: {}'.format(SEED))\n",
    "logger.info('REGRESSION: {}'.format(REGRESSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T20:08:20.401565Z",
     "iopub.status.busy": "2021-07-18T20:08:20.401565Z",
     "iopub.status.idle": "2021-07-18T20:08:20.417191Z",
     "shell.execute_reply": "2021-07-18T20:08:20.417191Z"
    }
   },
   "outputs": [],
   "source": [
    "SINCE = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T20:08:20.417191Z",
     "iopub.status.busy": "2021-07-18T20:08:20.417191Z",
     "iopub.status.idle": "2021-07-18T20:08:20.448443Z",
     "shell.execute_reply": "2021-07-18T20:08:20.448443Z"
    }
   },
   "outputs": [],
   "source": [
    "logger.debug('Start loading csv files ({:.3f} seconds passed)'.format(time.time() - SINCE))\n",
    "train, test, materials, techniques, sample_submission = load_csvfiles()\n",
    "logger.debug('Complete loading csv files ({:.3f} seconds passed)'.format(time.time() - SINCE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T20:08:20.448443Z",
     "iopub.status.busy": "2021-07-18T20:08:20.448443Z",
     "iopub.status.idle": "2021-07-18T20:08:20.464067Z",
     "shell.execute_reply": "2021-07-18T20:08:20.464067Z"
    }
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T20:08:20.464067Z",
     "iopub.status.busy": "2021-07-18T20:08:20.464067Z",
     "iopub.status.idle": "2021-07-18T20:08:20.479691Z",
     "shell.execute_reply": "2021-07-18T20:08:20.479691Z"
    }
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T20:10:01.964606Z",
     "iopub.status.busy": "2021-07-18T20:10:01.964606Z",
     "iopub.status.idle": "2021-07-18T20:10:01.980230Z",
     "shell.execute_reply": "2021-07-18T20:10:01.980230Z"
    }
   },
   "outputs": [],
   "source": [
    "train.set_index('object_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T20:10:01.994728Z",
     "iopub.status.busy": "2021-07-18T20:10:01.994728Z",
     "iopub.status.idle": "2021-07-19T01:53:32.057807Z",
     "shell.execute_reply": "2021-07-19T01:53:32.057807Z"
    }
   },
   "outputs": [],
   "source": [
    "fold_object_ids = load_cv_object_ids()\n",
    "for i, (train_object_ids, valid_object_ids) in enumerate(zip(fold_object_ids[0], fold_object_ids[1])):\n",
    "    assert(set(train_object_ids) & set(valid_object_ids) == set())\n",
    "    num_fold = i + 1\n",
    "    logger.debug('Start fold {} ({:.3f} seconds passed)'.format(num_fold, time.time() - SINCE))\n",
    "\n",
    "    # Separate dataset into training/validation fold\n",
    "    y_train = train.loc[train_object_ids, TARGET].values\n",
    "    y_valid = train.loc[valid_object_ids, TARGET].values\n",
    "    \n",
    "    # Training\n",
    "    logger.debug('Start training model ({:.3f} seconds passed)'.format(time.time() - SINCE))\n",
    "    ## Prepare model\n",
    "    model, input_size = initialize_model(MODEL_NAME)\n",
    "    ## Prepare transformers\n",
    "    train_transformer = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_transformer = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    # Prepare dataset\n",
    "    train_dataset = AtmaImageDatasetV02(train_object_ids, train_transformer, y_train)\n",
    "    val_dataset = AtmaImageDatasetV02(valid_object_ids, val_transformer, y_valid)\n",
    "    # Prepare dataloader\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=os.cpu_count()),\n",
    "        'val': DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=os.cpu_count()),\n",
    "    }\n",
    "    ## train estimator\n",
    "    estimator, train_losses, valid_losses = train_model(\n",
    "        model, dataloaders, criterion=nn.MSELoss(), num_epochs=NUM_EPOCHS,\n",
    "        optimizer=torch.optim.Adam(model.parameters()), log_func=logger.debug,\n",
    "        is_inception=MODEL_NAME == 'inception')\n",
    "    logger.debug('Complete training ({:.3f} seconds passed)'.format(time.time() - SINCE))\n",
    "    ## Visualize training loss\n",
    "    plt.plot(train_losses, label='train')\n",
    "    plt.plot(valid_losses, label='valid')\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=[1., 1.])\n",
    "    plt.title(f'Fold{num_fold}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save model and prediction\n",
    "    ## Prediction\n",
    "    if REGRESSION:\n",
    "        predictions = {}\n",
    "        for fold_, object_ids_ in zip(['train', 'val', 'test'],\n",
    "                                     [train_object_ids, valid_object_ids, test['object_id']]):\n",
    "            # Prepare transformer\n",
    "            transformer_ = transforms.Compose([\n",
    "                transforms.Resize(input_size),\n",
    "                transforms.CenterCrop(input_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            # Prepare dataset\n",
    "            dataset_ = AtmaImageDatasetV02(object_ids_, transformer_)\n",
    "            # Prepare dataloader\n",
    "            dataloader_ = DataLoader(dataset=dataset_, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                     num_workers=os.cpu_count())\n",
    "            # Prediction\n",
    "            predictions[fold_] = predict_by_model(estimator, dataloader_)\n",
    "        pred_train = pd.DataFrame(data=predictions['train'], columns=['pred'])\n",
    "        pred_valid = pd.DataFrame(data=predictions['val'], columns=['pred'])\n",
    "        pred_test = pd.DataFrame(data=predictions['test'], columns=['pred'])\n",
    "    else:  # Do not come here!\n",
    "        raise NotImplemented\n",
    "        try:\n",
    "            pred_train = pd.DataFrame(data=estimator.predict_proba(X_train),\n",
    "                                      columns=estimator.classes_)\n",
    "            pred_valid = pd.DataFrame(data=estimator.predict_proba(X_valid),\n",
    "                                      columns=estimator.classes_)\n",
    "            pred_test = pd.DataFrame(data=estimator.predict_proba(X_test),\n",
    "                                     columns=estimator.classes_)\n",
    "        except AttributeError:\n",
    "            pred_train = pd.DataFrame(data=estimator.decision_function(X_train),\n",
    "                                      columns=estimator.classes_)\n",
    "            pred_valid = pd.DataFrame(data=estimator.decision_function(X_valid),\n",
    "                                      columns=estimator.classes_)\n",
    "            pred_test = pd.DataFrame(data=estimator.decision_function(X_test),\n",
    "                                     columns=estimator.classes_)\n",
    "    ## Training set\n",
    "    pred_train['object_id'] = train_object_ids\n",
    "    filepath_fold_train = os.path.join(output_dir, f'cv_fold{num_fold}_training.csv')\n",
    "    pred_train.to_csv(filepath_fold_train, index=False)\n",
    "    logger.debug('Save training fold to {} ({:.3f} seconds passed)' \\\n",
    "                .format(filepath_fold_train, time.time() - SINCE))\n",
    "    ## Validation set\n",
    "    pred_valid['object_id'] = valid_object_ids\n",
    "    filepath_fold_valid = os.path.join(output_dir, f'cv_fold{num_fold}_validation.csv')\n",
    "    pred_valid.to_csv(filepath_fold_valid, index=False)\n",
    "    logger.debug('Save validation fold to {} ({:.3f} seconds passed)' \\\n",
    "                .format(filepath_fold_valid, time.time() - SINCE))\n",
    "    ## Test set\n",
    "    pred_test['object_id'] = test['object_id'].values\n",
    "    filepath_fold_test = os.path.join(output_dir, f'cv_fold{num_fold}_test.csv')\n",
    "    pred_test.to_csv(filepath_fold_test, index=False)\n",
    "    logger.debug('Save test result {} ({:.3f} seconds passed)' \\\n",
    "                 .format(filepath_fold_test, time.time() - SINCE))\n",
    "    ## Model\n",
    "    filepath_fold_model = os.path.join(output_dir, f'cv_fold{num_fold}_model.torch')\n",
    "    torch.save(estimator.state_dict(), filepath_fold_model)\n",
    "#     with open(filepath_fold_model, 'wb') as f:\n",
    "#         pickle.dump(estimator, f)\n",
    "    logger.debug('Save model {} ({:.3f} seconds passed)'.format(filepath_fold_model, time.time() - SINCE))\n",
    "    \n",
    "    # Save memory\n",
    "    del (estimator, y_train, y_valid, pred_train, pred_valid, pred_test)\n",
    "    gc.collect()\n",
    "\n",
    "    logger.debug('Complete fold {} ({:.3f} seconds passed)'.format(num_fold, time.time() - SINCE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.057807Z",
     "iopub.status.busy": "2021-07-19T01:53:32.057807Z",
     "iopub.status.idle": "2021-07-19T01:53:32.073436Z",
     "shell.execute_reply": "2021-07-19T01:53:32.073436Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse = partial(mean_squared_error, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.073436Z",
     "iopub.status.busy": "2021-07-19T01:53:32.073436Z",
     "iopub.status.idle": "2021-07-19T01:53:32.089063Z",
     "shell.execute_reply": "2021-07-19T01:53:32.089063Z"
    }
   },
   "outputs": [],
   "source": [
    "# qwk = partial(cohen_kappa_score, labels=np.sort(train['target'].unique()), weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.089063Z",
     "iopub.status.busy": "2021-07-19T01:53:32.089063Z",
     "iopub.status.idle": "2021-07-19T01:53:32.104732Z",
     "shell.execute_reply": "2021-07-19T01:53:32.104732Z"
    }
   },
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def predict(proba_0: float, proba_1: float, proba_2: float, proba_3: float) -> int:\n",
    "    return np.argmax((proba_0, proba_1, proba_2, proba_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.104732Z",
     "iopub.status.busy": "2021-07-19T01:53:32.104732Z",
     "iopub.status.idle": "2021-07-19T01:53:32.120309Z",
     "shell.execute_reply": "2021-07-19T01:53:32.120309Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.120309Z",
     "iopub.status.busy": "2021-07-19T01:53:32.120309Z",
     "iopub.status.idle": "2021-07-19T01:53:32.229684Z",
     "shell.execute_reply": "2021-07-19T01:53:32.229684Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_train_dfs = []\n",
    "for i in range(N_SPLITS):\n",
    "    num_fold = i + 1\n",
    "    logger.debug('Evaluate cv result (training set) Fold {}'.format(num_fold))\n",
    "    # Read cv result\n",
    "    filepath_fold_train = os.path.join(output_dir, f'cv_fold{num_fold}_training.csv')\n",
    "    pred_train_df = pd.read_csv(filepath_fold_train)\n",
    "    pred_train_df['actual'] = train.loc[pred_train_df['object_id'], TARGET].values\n",
    "    if REGRESSION:\n",
    "        if TARGET == 'target':\n",
    "            pred_train_df['pred'].clip(lower=0, upper=3, inplace=True)\n",
    "        else:\n",
    "            pred_train_df['pred'] = np.vectorize(soring_date2target)(pred_train_df['pred'])\n",
    "            pred_train_df['actual'] = np.vectorize(soring_date2target)(pred_train_df['actual'])\n",
    "    else:\n",
    "        pred_train_df['pred'] = predict(pred_train_df['0'], pred_train_df['1'],\n",
    "                                        pred_train_df['2'], pred_train_df['3'])\n",
    "    if not (REGRESSION and TARGET == 'target'):\n",
    "        print(confusion_matrix(pred_train_df['actual'], pred_train_df['pred'],\n",
    "                               labels=np.sort(train['target'].unique())))\n",
    "    loss = rmse(pred_train_df['actual'], pred_train_df['pred'])\n",
    "#     score = qwk(pred_train_df['actual'], pred_train_df['pred'])\n",
    "    logger.debug('Loss: {}'.format(loss))\n",
    "#     logger.debug('Score: {}'.format(score))\n",
    "    metrics['train_losses'].append(loss)\n",
    "#     metrics['train_scores'].append(score)\n",
    "    pred_train_dfs.append(pred_train_df)\n",
    "\n",
    "metrics['train_losses_avg'] = np.mean(metrics['train_losses'])\n",
    "metrics['train_losses_std'] = np.std(metrics['train_losses'])\n",
    "# metrics['train_scores_avg'] = np.mean(metrics['train_scores'])\n",
    "# metrics['train_scores_std'] = np.std(metrics['train_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.229684Z",
     "iopub.status.busy": "2021-07-19T01:53:32.229684Z",
     "iopub.status.idle": "2021-07-19T01:53:32.260934Z",
     "shell.execute_reply": "2021-07-19T01:53:32.260934Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_train = pd.concat(pred_train_dfs).groupby('object_id').sum()\n",
    "pred_train = pred_train / N_SPLITS\n",
    "if not REGRESSION:\n",
    "    pred_train['pred'] = predict(pred_train['0'], pred_train['1'], pred_train['2'], pred_train['3'])\n",
    "pred_train['actual'] = train.loc[pred_train.index, TARGET].values\n",
    "if REGRESSION and TARGET == 'sorting_date':\n",
    "    pred_train['actual'] = np.vectorize(soring_date2target)(pred_train['actual'])\n",
    "# for c in ('pred', 'actual'):\n",
    "#     pred_train[c] = pred_train[c].astype('int')\n",
    "pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.260934Z",
     "iopub.status.busy": "2021-07-19T01:53:32.260934Z",
     "iopub.status.idle": "2021-07-19T01:53:32.276559Z",
     "shell.execute_reply": "2021-07-19T01:53:32.276559Z"
    }
   },
   "outputs": [],
   "source": [
    "if not (REGRESSION and TARGET == 'target'):\n",
    "    print(confusion_matrix(pred_train['actual'], pred_train['pred'], labels=np.sort(train['target'].unique())))\n",
    "loss = rmse(pred_train['actual'], pred_train['pred'])\n",
    "# score = qwk(pred_train['actual'], pred_train['pred'])\n",
    "metrics['train_loss'] = loss\n",
    "# metrics['train_score'] = score\n",
    "logger.info('Training loss: {}'.format(loss))\n",
    "# logger.info('Training score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.276559Z",
     "iopub.status.busy": "2021-07-19T01:53:32.276559Z",
     "iopub.status.idle": "2021-07-19T01:53:32.292186Z",
     "shell.execute_reply": "2021-07-19T01:53:32.292186Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_train.to_csv(os.path.join(output_dir, 'prediction_train.csv'))\n",
    "logger.debug('Write cv result to {}'.format(os.path.join(output_dir, 'prediction_train.csv')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.292186Z",
     "iopub.status.busy": "2021-07-19T01:53:32.292186Z",
     "iopub.status.idle": "2021-07-19T01:53:32.370361Z",
     "shell.execute_reply": "2021-07-19T01:53:32.370361Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_valid_dfs = []\n",
    "for i in range(N_SPLITS):\n",
    "    num_fold = i + 1\n",
    "    logger.debug('Evaluate cv result (validation set) Fold {}'.format(num_fold))\n",
    "    # Read cv result\n",
    "    filepath_fold_valid = os.path.join(output_dir, f'cv_fold{num_fold}_validation.csv')\n",
    "    pred_valid_df = pd.read_csv(filepath_fold_valid)\n",
    "    pred_valid_df['actual'] = train.loc[pred_valid_df['object_id'], TARGET].values\n",
    "    if REGRESSION:\n",
    "        if TARGET == 'target':\n",
    "            pred_valid_df['pred'].clip(lower=0, upper=3, inplace=True)\n",
    "        else:\n",
    "            pred_valid_df['pred'] = np.vectorize(soring_date2target)(pred_valid_df['pred'])\n",
    "            pred_valid_df['actual'] = np.vectorize(soring_date2target)(pred_valid_df['actual'])\n",
    "    else:\n",
    "        pred_valid_df['pred'] = predict(pred_valid_df['0'], pred_valid_df['1'],\n",
    "                                        pred_valid_df['2'], pred_valid_df['3'])        \n",
    "    \n",
    "    if not (REGRESSION and TARGET == 'target'):\n",
    "        print(confusion_matrix(pred_valid_df['actual'], pred_valid_df['pred'],\n",
    "                               labels=np.sort(train['target'].unique())))\n",
    "    loss = rmse(pred_valid_df['actual'], pred_valid_df['pred'])\n",
    "#     score = qwk(pred_valid_df['actual'], pred_valid_df['pred'])\n",
    "    logger.debug('Loss: {}'.format(loss))\n",
    "#     logger.debug('Score: {}'.format(score))\n",
    "    metrics['valid_losses'].append(loss)\n",
    "#     metrics['valid_scores'].append(score)\n",
    "    pred_valid_dfs.append(pred_valid_df)\n",
    "    \n",
    "metrics['valid_losses_avg'] = np.mean(metrics['valid_losses'])\n",
    "metrics['valid_losses_std'] = np.std(metrics['valid_losses'])\n",
    "# metrics['valid_scores_avg'] = np.mean(metrics['valid_scores'])\n",
    "# metrics['valid_scores_std'] = np.std(metrics['valid_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.370361Z",
     "iopub.status.busy": "2021-07-19T01:53:32.370361Z",
     "iopub.status.idle": "2021-07-19T01:53:32.401610Z",
     "shell.execute_reply": "2021-07-19T01:53:32.401610Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_valid = pd.concat(pred_valid_dfs).groupby('object_id').sum()\n",
    "pred_valid = pred_valid / N_SPLITS\n",
    "if not REGRESSION:\n",
    "    pred_valid['pred'] = predict(pred_valid['0'], pred_valid['1'], pred_valid['2'], pred_valid['3'])\n",
    "pred_valid['actual'] = train.loc[pred_valid.index, TARGET].values\n",
    "if REGRESSION and TARGET == 'sorting_date':\n",
    "    pred_valid['actual'] = np.vectorize(soring_date2target)(pred_valid['actual'])\n",
    "# for c in ('pred', 'actual'):\n",
    "#     pred_valid[c] = pred_valid[c].astype('int')\n",
    "pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.401610Z",
     "iopub.status.busy": "2021-07-19T01:53:32.401610Z",
     "iopub.status.idle": "2021-07-19T01:53:32.417184Z",
     "shell.execute_reply": "2021-07-19T01:53:32.417184Z"
    }
   },
   "outputs": [],
   "source": [
    "if not REGRESSION:\n",
    "    print(confusion_matrix(pred_valid['actual'], pred_valid['pred'], labels=np.sort(train['target'].unique())))\n",
    "loss = rmse(pred_valid['actual'], pred_valid['pred'])\n",
    "# score = qwk(pred_valid['actual'], pred_valid['pred'])\n",
    "metrics['valid_loss'] = loss\n",
    "# metrics['valid_score'] = score\n",
    "logger.info('Validatino loss: {}'.format(loss))\n",
    "# logger.info('Validatino score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.417184Z",
     "iopub.status.busy": "2021-07-19T01:53:32.417184Z",
     "iopub.status.idle": "2021-07-19T01:53:32.432810Z",
     "shell.execute_reply": "2021-07-19T01:53:32.432810Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_valid.to_csv(os.path.join(output_dir, 'prediction_valid.csv'))\n",
    "logger.debug('Write cv result to {}'.format(os.path.join(output_dir, 'prediction_valid.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.432810Z",
     "iopub.status.busy": "2021-07-19T01:53:32.432810Z",
     "iopub.status.idle": "2021-07-19T01:53:32.448438Z",
     "shell.execute_reply": "2021-07-19T01:53:32.448438Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, 'metrics.json'), 'w') as f:\n",
    "    json.dump(dict(metrics), f)\n",
    "logger.debug('Write metrics to {}'.format(os.path.join(output_dir, 'metrics.json')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.448438Z",
     "iopub.status.busy": "2021-07-19T01:53:32.448438Z",
     "iopub.status.idle": "2021-07-19T01:53:32.526561Z",
     "shell.execute_reply": "2021-07-19T01:53:32.526561Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_test_dfs = []\n",
    "for i in range(N_SPLITS):\n",
    "    num_fold = i + 1\n",
    "    # Read cv result\n",
    "    filepath_fold_test = os.path.join(output_dir, f'cv_fold{num_fold}_test.csv')\n",
    "    pred_test_df = pd.read_csv(filepath_fold_test)\n",
    "    pred_test_dfs.append(pred_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.526561Z",
     "iopub.status.busy": "2021-07-19T01:53:32.526561Z",
     "iopub.status.idle": "2021-07-19T01:53:32.557807Z",
     "shell.execute_reply": "2021-07-19T01:53:32.557807Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_test = pd.concat(pred_test_dfs).groupby('object_id').sum()\n",
    "pred_test = pred_test / N_SPLITS\n",
    "if REGRESSION:\n",
    "    if TARGET == 'target':\n",
    "        pred_test['pred'].clip(lower=0, upper=3, inplace=True)\n",
    "    else:\n",
    "        pred_test['pred'] = np.vectorize(soring_date2target)(pred_test['pred'])\n",
    "else:\n",
    "    pred_test['pred'] = predict(pred_test['0'], pred_test['1'], pred_test['2'], pred_test['3'])\n",
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.557807Z",
     "iopub.status.busy": "2021-07-19T01:53:32.557807Z",
     "iopub.status.idle": "2021-07-19T01:53:32.573434Z",
     "shell.execute_reply": "2021-07-19T01:53:32.573434Z"
    }
   },
   "outputs": [],
   "source": [
    "test['target'] = pred_test.loc[test['object_id'], 'pred'].values\n",
    "test = test[['target']]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.573434Z",
     "iopub.status.busy": "2021-07-19T01:53:32.573434Z",
     "iopub.status.idle": "2021-07-19T01:53:32.589059Z",
     "shell.execute_reply": "2021-07-19T01:53:32.589059Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.589059Z",
     "iopub.status.busy": "2021-07-19T01:53:32.589059Z",
     "iopub.status.idle": "2021-07-19T01:53:32.620310Z",
     "shell.execute_reply": "2021-07-19T01:53:32.620310Z"
    }
   },
   "outputs": [],
   "source": [
    "test.to_csv(os.path.join(output_dir, f'{str(EXP_NO).zfill(3)}_submission.csv'), index=False)\n",
    "logger.debug('Write submission to {}'.format(os.path.join(output_dir, f'{str(EXP_NO).zfill(3)}_submission.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.620310Z",
     "iopub.status.busy": "2021-07-19T01:53:32.620310Z",
     "iopub.status.idle": "2021-07-19T01:53:32.824346Z",
     "shell.execute_reply": "2021-07-19T01:53:32.824346Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "if not (REGRESSION and TARGET == 'target'):\n",
    "    sns.countplot(data=test, x='target')\n",
    "else:\n",
    "    sns.histplot(data=test, x='target')\n",
    "sns.despine()\n",
    "fig.savefig(os.path.join(output_dir, 'prediction.png'))\n",
    "logger.debug('Write figure to {}'.format(os.path.join(output_dir, 'prediction.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T01:53:32.824346Z",
     "iopub.status.busy": "2021-07-19T01:53:32.824346Z",
     "iopub.status.idle": "2021-07-19T01:53:32.839975Z",
     "shell.execute_reply": "2021-07-19T01:53:32.839975Z"
    }
   },
   "outputs": [],
   "source": [
    "logger.debug('Complete ({:.3f} seconds passed)'.format(time.time() - SINCE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
